{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1df8229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries & Config Loaded\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS & SETUP\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sklearn - Preprocessing & Selection\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.cluster import KMeans  # <--- NEW: For Clustering\n",
    "\n",
    "# Sklearn - Models & Metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, f1_score, precision_score, recall_score,\n",
    "    accuracy_score, classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = Path(\".\")  # Update to your data path\n",
    "KEY_COLS = [\"student_id\", \"code_module\", \"code_presentation\"]\n",
    "TARGET_COL = \"target_pass\"\n",
    "PASS_MARK = 40\n",
    "\n",
    "# Leakage columns to drop later\n",
    "LEAKAGE_COLS = [\n",
    "    \"final_result\", \"date_unregistration\", \"target_score\", \n",
    "    \"target_score_norm\", \"weight_covered\", \"target_score_x\", \"target_score_y\", \"target_pass\"\n",
    "]\n",
    "\n",
    "print(\"✓ Libraries & Config Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c0d3b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading OULAD tables...\n",
      "✓ Data Loaded\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD DATA\n",
    "# =============================================================================\n",
    "def load_csv(name: str) -> pd.DataFrame:\n",
    "    return pd.read_csv(DATA_DIR / name)\n",
    "\n",
    "print(\"Loading OULAD tables...\")\n",
    "assessments          = load_csv(\"./assessments.csv\")\n",
    "courses              = load_csv(\"./courses.csv\")\n",
    "student_assessment   = load_csv(\"./studentAssessment.csv\")\n",
    "student_info         = load_csv(\"./studentInfo.csv\")\n",
    "student_registration = load_csv(\"./studentRegistration.csv\")\n",
    "student_vle          = load_csv(\"./studentVle.csv\")\n",
    "vle                  = load_csv(\"./vle.csv\")\n",
    "\n",
    "# Standardize IDs\n",
    "for df in [student_assessment, student_info, student_registration, student_vle]:\n",
    "    df.rename(columns={\"id_student\": \"student_id\"}, inplace=True)\n",
    "\n",
    "print(\"✓ Data Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41f72d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Base Features...\n",
      "Building Temporal VLE Features...\n",
      "✓ Temporal Features Built: (1303720, 11)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 1. BASE FEATURES (Demographics & Registration)\n",
    "# =============================================================================\n",
    "print(\"Building Base Features...\")\n",
    "\n",
    "# Demographics\n",
    "demographics = student_info[KEY_COLS + [\n",
    "    \"gender\", \"region\", \"highest_education\", \"imd_band\",\n",
    "    \"age_band\", \"num_of_prev_attempts\", \"studied_credits\", \"disability\", \"final_result\"\n",
    "]].copy()\n",
    "\n",
    "# Fix Targets (Binary Class + Regression)\n",
    "demographics[\"target_pass\"] = demographics[\"final_result\"].isin([\"Pass\", \"Distinction\"]).astype(int)\n",
    "\n",
    "# Score Aggregation (Fixes the duplicate merge bug)\n",
    "score_df = student_assessment.merge(\n",
    "    assessments[[\"id_assessment\", \"code_module\", \"code_presentation\", \"weight\"]], \n",
    "    on=\"id_assessment\", how=\"left\"\n",
    ")\n",
    "score_agg = (\n",
    "    score_df.groupby(KEY_COLS)\n",
    "    .apply(lambda g: pd.Series({\n",
    "        \"target_score\": np.average(g[\"score\"].fillna(0), weights=g[\"weight\"].fillna(1)) \n",
    "                        if g[\"weight\"].sum() > 0 else np.nan\n",
    "    })).reset_index()\n",
    ")\n",
    "\n",
    "# Merge targets ONCE (This fixes the bug!)\n",
    "demographics = demographics.merge(score_agg, on=KEY_COLS, how=\"left\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. TEMPORAL VLE FEATURES (Velocity & Recency)\n",
    "# =============================================================================\n",
    "print(\"Building Temporal VLE Features...\")\n",
    "\n",
    "# Prepare VLE interactions\n",
    "vle_interactions = student_vle.merge(\n",
    "    vle[[\"id_site\", \"code_module\", \"code_presentation\", \"activity_type\"]],\n",
    "    on=[\"id_site\", \"code_module\", \"code_presentation\"], how=\"left\"\n",
    ")\n",
    "vle_interactions[\"week\"] = (vle_interactions[\"date\"] // 7).astype(int)\n",
    "vle_interactions = vle_interactions[vle_interactions[\"week\"] >= 0]\n",
    "\n",
    "# Aggregate by Student-Week\n",
    "weekly_vle = vle_interactions.groupby(KEY_COLS + [\"week\"]).agg(\n",
    "    weekly_clicks=(\"sum_click\", \"sum\"),\n",
    "    active_days=(\"date\", \"nunique\")\n",
    ").reset_index()\n",
    "\n",
    "# Create a Skeleton (Student x Week) to handle weeks with ZERO activity\n",
    "# This is crucial for \"Weeks Since Last Login\"\n",
    "unique_students = demographics[KEY_COLS].drop_duplicates()\n",
    "weeks = range(0, 40) # Course length approx 39 weeks\n",
    "skeleton = pd.concat([unique_students.assign(week=w) for w in weeks], ignore_index=True)\n",
    "\n",
    "# Merge actual data into skeleton (fill missing weeks with 0)\n",
    "ews_df = skeleton.merge(weekly_vle, on=KEY_COLS + [\"week\"], how=\"left\")\n",
    "ews_df[[\"weekly_clicks\", \"active_days\"]] = ews_df[[\"weekly_clicks\", \"active_days\"]].fillna(0)\n",
    "\n",
    "# Sort for rolling calculations\n",
    "ews_df = ews_df.sort_values(KEY_COLS + [\"week\"])\n",
    "\n",
    "# --- NEW FEATURE: CUMULATIVE ---\n",
    "ews_df[\"cum_clicks\"] = ews_df.groupby(KEY_COLS)[\"weekly_clicks\"].cumsum()\n",
    "\n",
    "# --- NEW FEATURE: VELOCITY (Trend) ---\n",
    "# Difference in clicks from previous week\n",
    "ews_df[\"clicks_velocity\"] = ews_df.groupby(KEY_COLS)[\"weekly_clicks\"].diff().fillna(0)\n",
    "\n",
    "# --- NEW FEATURE: RECENCY (Weeks since last activity) ---\n",
    "# If active_days > 0, set current week. Forward fill this, then subtract from current week.\n",
    "ews_df[\"last_active_week\"] = ews_df[\"week\"].where(ews_df[\"active_days\"] > 0)\n",
    "ews_df[\"last_active_week\"] = ews_df.groupby(KEY_COLS)[\"last_active_week\"].ffill().fillna(-1)\n",
    "ews_df[\"weeks_since_last_activity\"] = ews_df[\"week\"] - ews_df[\"last_active_week\"]\n",
    "\n",
    "# --- NEW FEATURE: COHORT RELATIVE PERFORMANCE ---\n",
    "# Compare student clicks to the average of their module-presentation cohort\n",
    "cohort_means = ews_df.groupby([\"code_module\", \"code_presentation\", \"week\"])[\"cum_clicks\"].transform(\"mean\")\n",
    "ews_df[\"clicks_vs_cohort\"] = ews_df[\"cum_clicks\"] / (cohort_means + 1) # +1 to avoid div/0\n",
    "\n",
    "print(f\"✓ Temporal Features Built: {ews_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "832d7148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Clustering...\n",
      "✓ Clustering Completed & Merged\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 3. UNSUPERVISED CLUSTERING (Student Segmentation)\n",
    "# =============================================================================\n",
    "print(\"Running Clustering...\")\n",
    "\n",
    "# We will cluster students based on their behavior pattern (Average Clicks & Variability)\n",
    "cluster_data = ews_df.groupby(KEY_COLS).agg({\n",
    "    \"weekly_clicks\": [\"mean\", \"std\", \"max\"],\n",
    "    \"active_days\": \"mean\"\n",
    "}).fillna(0)\n",
    "cluster_data.columns = [\"_\".join(col) for col in cluster_data.columns]\n",
    "\n",
    "# Scaling\n",
    "scaler_cl = StandardScaler()\n",
    "X_cluster = scaler_cl.fit_transform(cluster_data)\n",
    "\n",
    "# K-Means (4 Clusters: e.g., High Flyers, Steady, Struggling, Ghosts)\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "cluster_data[\"cluster_id\"] = kmeans.fit_predict(X_cluster)\n",
    "\n",
    "# Merge Cluster ID back into main EWS dataframe\n",
    "ews_df = ews_df.merge(cluster_data[\"cluster_id\"], on=KEY_COLS, how=\"left\")\n",
    "\n",
    "# One-Hot Encode the Cluster ID for the Supervised Model\n",
    "ews_df = pd.get_dummies(ews_df, columns=[\"cluster_id\"], prefix=\"cluster\")\n",
    "\n",
    "print(\"✓ Clustering Completed & Merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a982cabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FINAL FEATURE STORE READY\n",
      "Shape: (1271127, 26)\n",
      "Columns: ['student_id', 'code_module', 'code_presentation', 'week', 'weekly_clicks', 'active_days', 'cum_clicks', 'clicks_velocity', 'last_active_week', 'weeks_since_last_activity', 'clicks_vs_cohort', 'cluster_0', 'cluster_1', 'cluster_2', 'cluster_3', 'gender', 'region', 'highest_education', 'imd_band', 'age_band', 'num_of_prev_attempts', 'studied_credits', 'disability', 'final_result', 'target_pass', 'target_score']\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 4. FINAL MERGE & CLEANING\n",
    "# =============================================================================\n",
    "\n",
    "# Merge Demographics\n",
    "feature_store = ews_df.merge(demographics, on=KEY_COLS, how=\"left\")\n",
    "\n",
    "# Filter: Only weeks 0 to 38 (standard OULAD length)\n",
    "feature_store = feature_store[(feature_store[\"week\"] >= 0) & (feature_store[\"week\"] <= 38)]\n",
    "\n",
    "# Impute Missing Values\n",
    "# IMD Band -> Mode\n",
    "feature_store[\"imd_band\"] = feature_store[\"imd_band\"].fillna(feature_store[\"imd_band\"].mode()[0])\n",
    "# Age Band -> Mode\n",
    "feature_store[\"age_band\"] = feature_store[\"age_band\"].fillna(feature_store[\"age_band\"].mode()[0])\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL FEATURE STORE READY\")\n",
    "print(f\"Shape: {feature_store.shape}\")\n",
    "print(f\"Columns: {list(feature_store.columns)}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1aaf38ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training for Week 5 ---\n",
      "Accuracy: 0.7259\n",
      "ROC AUC:  0.8035\n",
      "\n",
      "Top 5 Predictors:\n",
      "clicks_vs_cohort    0.155136\n",
      "cum_clicks          0.121007\n",
      "weekly_clicks       0.108594\n",
      "clicks_velocity     0.090902\n",
      "active_days         0.067936\n",
      "dtype: float64\n",
      "\n",
      "--- Training for Week 10 ---\n",
      "Accuracy: 0.7640\n",
      "ROC AUC:  0.8444\n",
      "\n",
      "Top 5 Predictors:\n",
      "clicks_vs_cohort             0.160643\n",
      "cum_clicks                   0.119714\n",
      "last_active_week             0.095663\n",
      "clicks_velocity              0.085349\n",
      "weeks_since_last_activity    0.082028\n",
      "dtype: float64\n",
      "\n",
      "--- Training for Week 20 ---\n",
      "Accuracy: 0.8448\n",
      "ROC AUC:  0.9150\n",
      "\n",
      "Top 5 Predictors:\n",
      "last_active_week             0.151124\n",
      "clicks_vs_cohort             0.141537\n",
      "weeks_since_last_activity    0.123668\n",
      "weekly_clicks                0.109970\n",
      "active_days                  0.094397\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 5. MODEL TRAINING (Temporal Split)\n",
    "# =============================================================================\n",
    "\n",
    "def train_at_week(week_num):\n",
    "    print(f\"\\n--- Training for Week {week_num} ---\")\n",
    "    \n",
    "    # 1. Snapshot: Take data only up to this week\n",
    "    # For our temporal structure, we just take the row corresponding to this week\n",
    "    # because it contains cumulative data + current velocity.\n",
    "    df_week = feature_store[feature_store[\"week\"] == week_num].copy()\n",
    "    \n",
    "    # 2. Define Features & Target\n",
    "    drop_cols = KEY_COLS + LEAKAGE_COLS + [\"week\"]\n",
    "    X = df_week.drop(columns=[c for c in drop_cols if c in df_week.columns])\n",
    "    y = df_week[\"target_pass\"]\n",
    "    \n",
    "    # 3. Preprocessing\n",
    "    # Identify Cat/Num columns\n",
    "    cat_cols = X.select_dtypes(include=['object']).columns\n",
    "    num_cols = X.select_dtypes(include=['number']).columns\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', StandardScaler(), num_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
    "    ])\n",
    "    \n",
    "    # 4. Split (GroupShuffleSplit to prevent student leakage)\n",
    "    splitter = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    train_idx, test_idx = next(splitter.split(X, y, groups=df_week[\"student_id\"]))\n",
    "    \n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "    # 5. Pipeline & Train\n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # 6. Evaluate\n",
    "    preds = model.predict(X_test)\n",
    "    probs = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    auc = roc_auc_score(y_test, probs)\n",
    "    \n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"ROC AUC:  {auc:.4f}\")\n",
    "    \n",
    "    # Feature Importance (Optional)\n",
    "    if hasattr(model['classifier'], 'feature_importances_'):\n",
    "        # Get feature names after one-hot encoding\n",
    "        ohe = model['preprocessor'].named_transformers_['cat']\n",
    "        ohe_cols = ohe.get_feature_names_out(cat_cols)\n",
    "        all_cols = list(num_cols) + list(ohe_cols)\n",
    "        \n",
    "        importances = pd.Series(model['classifier'].feature_importances_, index=all_cols)\n",
    "        print(\"\\nTop 5 Predictors:\")\n",
    "        print(importances.sort_values(ascending=False).head(5))\n",
    "\n",
    "# --- Run for key intervention points ---\n",
    "for w in [5, 10, 20]:\n",
    "    train_at_week(w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faidm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
