{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAIDM Group Project: Student Performance Prediction & Clustering\n",
    "## Open University Learning Analytics Dataset (OULAD)\n",
    "\n",
    "**Module:** WM9QG-15 Fundamentals of AI and Data Mining  \n",
    "**Methodology:** CRISP-DM  \n",
    "**Deadline:** W/C 26th January 2026  \n",
    "\n",
    "---\n",
    "\n",
    "### Project Tasks:\n",
    "1. **Predictive Model (Supervised ML):** Predict students' final outcomes based on demographics, VLE engagement, and assessment data\n",
    "2. **Clustering Model (Unsupervised ML):** Segment students into meaningful groups based on engagement patterns\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Business Understanding (CRISP-DM Phase 1)\n",
    "\n",
    "### 1.1 Problem Statement\n",
    "The Open University wants to:\n",
    "- **Identify at-risk students early** for intervention\n",
    "- **Understand student engagement patterns** to inform teaching strategies\n",
    "- **Predict final module outcomes** to enable personalised support\n",
    "\n",
    "### 1.2 Success Criteria\n",
    "- Build a predictive model with acceptable accuracy (target: >70% or meaningful AUC-ROC)\n",
    "- Identify actionable student segments that can inform intervention strategies\n",
    "- Deliverables must follow CRISP-DM methodology and be presentation-ready\n",
    "\n",
    "### 1.3 Business Questions\n",
    "1. Which students are at risk of failing or withdrawing?\n",
    "2. What engagement patterns characterise successful vs struggling students?\n",
    "3. Can we intervene early enough to make a difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Understanding (CRISP-DM Phase 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data path - UPDATE THIS TO YOUR LOCAL PATH\n",
    "DATA_PATH = Path('.')  # Change to your data directory\n",
    "\n",
    "# Load all datasets\n",
    "print(\"Loading datasets...\\n\")\n",
    "\n",
    "student_info = pd.read_csv(DATA_PATH / 'studentInfo.csv')\n",
    "student_assessment = pd.read_csv(DATA_PATH / 'studentAssessment.csv')\n",
    "assessments = pd.read_csv(DATA_PATH / 'assessments.csv')\n",
    "courses = pd.read_csv(DATA_PATH / 'courses.csv')\n",
    "student_registration = pd.read_csv(DATA_PATH / 'studentRegistration.csv')\n",
    "vle = pd.read_csv(DATA_PATH / 'vle.csv')\n",
    "student_vle = pd.read_csv(DATA_PATH / 'studentVle.csv')\n",
    "\n",
    "print(\"All datasets loaded successfully!\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display shape and basic info for each dataset\n",
    "datasets = {\n",
    "    'studentInfo': student_info,\n",
    "    'studentAssessment': student_assessment,\n",
    "    'assessments': assessments,\n",
    "    'courses': courses,\n",
    "    'studentRegistration': student_registration,\n",
    "    'vle': vle,\n",
    "    'studentVle': student_vle\n",
    "}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Shape: {df.shape[0]:,} rows x {df.shape[1]} columns\")\n",
    "    print(f\"  Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed look at studentInfo\n",
    "print(\"=\"*60)\n",
    "print(\"STUDENT INFO - DETAILED ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTotal student-module registrations: {len(student_info):,}\")\n",
    "print(f\"Unique students: {student_info['id_student'].nunique():,}\")\n",
    "print(f\"Unique modules: {student_info['code_module'].nunique()}\")\n",
    "print(f\"Unique presentations: {student_info['code_presentation'].nunique()}\")\n",
    "\n",
    "print(\"\\n--- Data Types ---\")\n",
    "print(student_info.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values analysis\n",
    "print(\"=\"*60)\n",
    "print(\"MISSING VALUE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df) * 100).round(2)\n",
    "    if missing.any():\n",
    "        print(f\"\\n{name}:\")\n",
    "        for col in missing[missing > 0].index:\n",
    "            print(f\"  {col}: {missing[col]:,} ({missing_pct[col]}%)\")\n",
    "    else:\n",
    "        print(f\"\\n{name}: No missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable analysis\n",
    "print(\"=\"*60)\n",
    "print(\"TARGET VARIABLE: final_result\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "result_counts = student_info['final_result'].value_counts()\n",
    "result_pct = student_info['final_result'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"\\nDistribution:\")\n",
    "for result in result_counts.index:\n",
    "    print(f\"  {result}: {result_counts[result]:,} ({result_pct[result]:.1f}%)\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "colors = ['#2ecc71', '#3498db', '#e74c3c', '#95a5a6']\n",
    "result_counts.plot(kind='bar', ax=axes[0], color=colors, edgecolor='black')\n",
    "axes[0].set_title('Student Final Results Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Final Result')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "axes[1].pie(result_counts, labels=result_counts.index, autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "axes[1].set_title('Final Results Proportion', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: Class imbalance present - consider stratified sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variables exploration\n",
    "print(\"=\"*60)\n",
    "print(\"CATEGORICAL VARIABLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "categorical_cols = ['gender', 'region', 'highest_education', 'imd_band', 'age_band', 'disability']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(student_info[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize demographics\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(categorical_cols):\n",
    "    student_info[col].value_counts().plot(kind='bar', ax=axes[i], color='steelblue', edgecolor='black')\n",
    "    axes[i].set_title(f'{col} Distribution', fontsize=11, fontweight='bold')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "    axes[i].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final result by demographics\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(categorical_cols):\n",
    "    ct = pd.crosstab(student_info[col], student_info['final_result'], normalize='index') * 100\n",
    "    ct[['Pass', 'Distinction', 'Fail', 'Withdrawn']].plot(kind='bar', stacked=True, ax=axes[i],\n",
    "                                                          color=['#2ecc71', '#3498db', '#e74c3c', '#95a5a6'])\n",
    "    axes[i].set_title(f'Final Result by {col}', fontsize=11, fontweight='bold')\n",
    "    axes[i].set_ylabel('Percentage')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "    axes[i].legend(loc='upper right', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VLE Engagement Analysis\n",
    "print(\"=\"*60)\n",
    "print(\"VLE ENGAGEMENT ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nTotal VLE interaction records: {len(student_vle):,}\")\n",
    "print(f\"Total clicks recorded: {student_vle['sum_click'].sum():,}\")\n",
    "print(f\"Unique students with VLE activity: {student_vle['id_student'].nunique():,}\")\n",
    "print(f\"Date range: {student_vle['date'].min()} to {student_vle['date'].max()}\")\n",
    "print(\"  (negative = before course start)\")\n",
    "\n",
    "print(\"\\n--- Click Statistics ---\")\n",
    "print(student_vle['sum_click'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VLE Activity Types\n",
    "print(\"\\n--- VLE Activity Types ---\")\n",
    "print(vle['activity_type'].value_counts())\n",
    "\n",
    "# Clicks by activity type\n",
    "vle_with_type = student_vle.merge(vle[['id_site', 'activity_type']], on='id_site', how='left')\n",
    "clicks_by_type = vle_with_type.groupby('activity_type')['sum_click'].sum().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n--- Total Clicks by Activity Type ---\")\n",
    "print(clicks_by_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessment analysis\n",
    "print(\"=\"*60)\n",
    "print(\"ASSESSMENT DATA ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nTotal assessments in catalog: {len(assessments)}\")\n",
    "print(f\"Total submissions: {len(student_assessment):,}\")\n",
    "\n",
    "print(\"\\nAssessment types:\")\n",
    "print(assessments['assessment_type'].value_counts())\n",
    "\n",
    "print(\"\\n--- Score Statistics ---\")\n",
    "print(student_assessment['score'].describe())\n",
    "\n",
    "missing_scores = student_assessment['score'].isnull().sum()\n",
    "print(f\"\\nMissing scores: {missing_scores:,} ({missing_scores/len(student_assessment)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation (CRISP-DM Phase 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build unified dataset\n",
    "print(\"=\"*60)\n",
    "print(\"BUILDING UNIFIED DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df = student_info.copy()\n",
    "\n",
    "# Create unique key\n",
    "df['student_module_key'] = (df['id_student'].astype(str) + '_' + \n",
    "                            df['code_module'] + '_' + df['code_presentation'])\n",
    "\n",
    "print(f\"Base dataset: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge course info\n",
    "df = df.merge(courses, on=['code_module', 'code_presentation'], how='left')\n",
    "print(f\"After courses: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge registration and create features\n",
    "df = df.merge(student_registration, on=['code_module', 'code_presentation', 'id_student'], how='left')\n",
    "print(f\"After registration: {df.shape}\")\n",
    "\n",
    "df['registered_early'] = (df['date_registration'] < 0).astype(int)\n",
    "df['days_before_start'] = df['date_registration'].apply(lambda x: abs(x) if x < 0 else 0)\n",
    "df['withdrew'] = df['date_unregistration'].notna().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessment features\n",
    "print(\"\\n--- Creating Assessment Features ---\")\n",
    "\n",
    "assessments_with_meta = student_assessment.merge(assessments, on='id_assessment', how='left')\n",
    "assessments_with_meta['student_module_key'] = (assessments_with_meta['id_student'].astype(str) + '_' + \n",
    "                                                assessments_with_meta['code_module'] + '_' + \n",
    "                                                assessments_with_meta['code_presentation'])\n",
    "\n",
    "# Aggregate\n",
    "assessment_features = assessments_with_meta.groupby('student_module_key').agg({\n",
    "    'score': ['mean', 'std', 'min', 'max', 'count'],\n",
    "    'date_submitted': ['mean', 'std'],\n",
    "    'is_banked': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "assessment_features.columns = ['student_module_key', 'avg_score', 'score_std', 'min_score', 'max_score', \n",
    "                               'num_assessments_submitted', 'avg_submission_day', 'submission_day_std', \n",
    "                               'num_banked']\n",
    "\n",
    "# Timeliness\n",
    "assessments_with_meta['days_early'] = assessments_with_meta['date'] - assessments_with_meta['date_submitted']\n",
    "timeliness = assessments_with_meta.groupby('student_module_key').agg({\n",
    "    'days_early': ['mean', 'min']\n",
    "}).reset_index()\n",
    "timeliness.columns = ['student_module_key', 'avg_days_early', 'worst_days_early']\n",
    "\n",
    "assessment_features = assessment_features.merge(timeliness, on='student_module_key', how='left')\n",
    "\n",
    "print(f\"Assessment features: {assessment_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge assessment features\n",
    "df = df.merge(assessment_features, on='student_module_key', how='left')\n",
    "print(f\"After assessments: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessment type-specific features\n",
    "print(\"\\n--- Assessment Type Features ---\")\n",
    "\n",
    "for atype in ['TMA', 'CMA', 'Exam']:\n",
    "    type_data = assessments_with_meta[assessments_with_meta['assessment_type'] == atype]\n",
    "    type_scores = type_data.groupby('student_module_key').agg({'score': ['mean', 'count']}).reset_index()\n",
    "    type_scores.columns = ['student_module_key', f'{atype.lower()}_avg_score', f'{atype.lower()}_count']\n",
    "    df = df.merge(type_scores, on='student_module_key', how='left')\n",
    "    print(f\"  Added {atype} features\")\n",
    "\n",
    "print(f\"After assessment types: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VLE Engagement Features\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING VLE ENGAGEMENT FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "student_vle['student_module_key'] = (student_vle['id_student'].astype(str) + '_' + \n",
    "                                      student_vle['code_module'] + '_' + \n",
    "                                      student_vle['code_presentation'])\n",
    "\n",
    "# Basic engagement\n",
    "vle_features = student_vle.groupby('student_module_key').agg({\n",
    "    'sum_click': ['sum', 'mean', 'std', 'max'],\n",
    "    'date': ['min', 'max', 'nunique'],\n",
    "    'id_site': 'nunique'\n",
    "}).reset_index()\n",
    "\n",
    "vle_features.columns = ['student_module_key', 'total_clicks', 'avg_daily_clicks', \n",
    "                        'click_std', 'max_daily_clicks', 'first_access_day', \n",
    "                        'last_access_day', 'active_days', 'unique_resources']\n",
    "\n",
    "vle_features['engagement_span'] = vle_features['last_access_day'] - vle_features['first_access_day']\n",
    "vle_features['clicks_per_active_day'] = vle_features['total_clicks'] / vle_features['active_days'].replace(0, 1)\n",
    "\n",
    "print(f\"VLE features: {vle_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activity type clicks\n",
    "print(\"\\n--- Activity Type Click Features ---\")\n",
    "\n",
    "vle_with_type = student_vle.merge(vle[['id_site', 'activity_type']], on='id_site', how='left')\n",
    "activity_clicks = vle_with_type.groupby(['student_module_key', 'activity_type'])['sum_click'].sum().unstack(fill_value=0)\n",
    "activity_clicks = activity_clicks.add_prefix('clicks_').reset_index()\n",
    "\n",
    "print(f\"Activity types: {activity_clicks.shape[1] - 1}\")\n",
    "\n",
    "vle_features = vle_features.merge(activity_clicks, on='student_module_key', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early engagement (first 2 weeks)\n",
    "print(\"\\n--- Early Engagement Features ---\")\n",
    "\n",
    "early_vle = student_vle[student_vle['date'] <= 14]\n",
    "early_engagement = early_vle.groupby('student_module_key').agg({\n",
    "    'sum_click': 'sum',\n",
    "    'date': 'nunique',\n",
    "    'id_site': 'nunique'\n",
    "}).reset_index()\n",
    "early_engagement.columns = ['student_module_key', 'early_clicks', 'early_active_days', 'early_resources']\n",
    "\n",
    "vle_features = vle_features.merge(early_engagement, on='student_module_key', how='left')\n",
    "\n",
    "# Pre-course engagement\n",
    "pre_course = student_vle[student_vle['date'] < 0]\n",
    "pre_engagement = pre_course.groupby('student_module_key')['sum_click'].sum().reset_index()\n",
    "pre_engagement.columns = ['student_module_key', 'pre_course_clicks']\n",
    "\n",
    "vle_features = vle_features.merge(pre_engagement, on='student_module_key', how='left')\n",
    "\n",
    "print(f\"Final VLE features: {vle_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge VLE features\n",
    "df = df.merge(vle_features, on='student_module_key', how='left')\n",
    "print(f\"After VLE features: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA CLEANING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df['imd_band'] = df['imd_band'].fillna('Unknown')\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "df[numeric_cols] = df[numeric_cols].fillna(0)\n",
    "\n",
    "print(f\"Missing values after cleaning: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"\\n--- Encoding Categorical Variables ---\")\n",
    "\n",
    "df_encoded = df.copy()\n",
    "label_encoders = {}\n",
    "\n",
    "for col in ['gender', 'region', 'disability', 'code_module', 'code_presentation']:\n",
    "    le = LabelEncoder()\n",
    "    df_encoded[col + '_encoded'] = le.fit_transform(df_encoded[col])\n",
    "    label_encoders[col] = le\n",
    "    print(f\"  Encoded {col}: {len(le.classes_)} categories\")\n",
    "\n",
    "# Ordinal encode education and age\n",
    "education_order = ['No Formal quals', 'Lower Than A Level', 'A Level or Equivalent', \n",
    "                   'HE Qualification', 'Post Graduate Qualification']\n",
    "df_encoded['education_level'] = df_encoded['highest_education'].apply(\n",
    "    lambda x: education_order.index(x) if x in education_order else -1)\n",
    "\n",
    "age_order = ['0-35', '35-55', '55<=']\n",
    "df_encoded['age_level'] = df_encoded['age_band'].apply(\n",
    "    lambda x: age_order.index(x) if x in age_order else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target variables\n",
    "print(\"\\n--- Target Variables ---\")\n",
    "\n",
    "df_encoded['target_binary'] = df_encoded['final_result'].apply(\n",
    "    lambda x: 1 if x in ['Pass', 'Distinction'] else 0)\n",
    "\n",
    "result_mapping = {'Pass': 2, 'Distinction': 3, 'Fail': 1, 'Withdrawn': 0}\n",
    "df_encoded['target_multiclass'] = df_encoded['final_result'].map(result_mapping)\n",
    "\n",
    "print(\"\\nBinary (1=Pass/Distinction, 0=Fail/Withdrawn):\")\n",
    "print(df_encoded['target_binary'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final dataset\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL PREPARED DATASET\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nShape: {df_encoded.shape}\")\n",
    "print(f\"\\nColumns: {len(df_encoded.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "df_encoded.to_csv('prepared_student_data.csv', index=False)\n",
    "print(\"Saved to 'prepared_student_data.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modelling (CRISP-DM Phase 4)\n",
    "\n",
    "### 4.1 Predictive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score, \n",
    "                             precision_score, recall_score, f1_score, roc_auc_score, roc_curve)\n",
    "\n",
    "print(\"ML libraries loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "exclude_cols = ['id_student', 'student_module_key', 'final_result', 'target_binary', \n",
    "                'target_multiclass', 'code_module', 'code_presentation', 'gender', \n",
    "                'region', 'highest_education', 'imd_band', 'age_band', 'disability',\n",
    "                'date_registration', 'date_unregistration']\n",
    "\n",
    "feature_cols = [col for col in df_encoded.columns if col not in exclude_cols \n",
    "               and df_encoded[col].dtype in ['int64', 'float64', 'int32', 'float32']]\n",
    "\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = df_encoded[feature_cols].copy()\n",
    "y = df_encoded['target_binary'].copy()\n",
    "\n",
    "X = X.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Train: {X_train.shape[0]:,}\")\n",
    "print(f\"Test: {X_test.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    \n",
    "    if 'Logistic' in name:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    results[name] = {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1': f1_score(y_test, y_pred),\n",
    "        'AUC-ROC': roc_auc_score(y_test, y_proba)\n",
    "    }\n",
    "    \n",
    "    for metric, value in results[name].items():\n",
    "        print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results comparison\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(results_df.round(4))\n",
    "\n",
    "results_df.plot(kind='bar', figsize=(12, 5))\n",
    "plt.title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Analysis\n",
    "print(\"=\"*60)\n",
    "print(\"RANDOM FOREST - DETAILED\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "rf_model = models['Random Forest']\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf, target_names=['Fail/Withdrawn', 'Pass/Distinction']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_rf)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Fail/Withdrawn', 'Pass/Distinction'],\n",
    "            yticklabels=['Fail/Withdrawn', 'Pass/Distinction'])\n",
    "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 Features:\")\n",
    "print(feature_importance.head(20).to_string(index=False))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "top_20 = feature_importance.head(20)\n",
    "plt.barh(range(len(top_20)), top_20['importance'], color='steelblue')\n",
    "plt.yticks(range(len(top_20)), top_20['feature'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance (Top 20)', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for name, model in models.items():\n",
    "    if 'Logistic' in name:\n",
    "        y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.3f})', linewidth=2)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "print(\"=\"*60)\n",
    "print(\"HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "rf_tuned = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "grid_search = GridSearchCV(rf_tuned, param_grid, cv=5, scoring='roc_auc', n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest params: {grid_search.best_params_}\")\n",
    "print(f\"Best CV AUC: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred_tuned = best_rf.predict(X_test)\n",
    "y_proba_tuned = best_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f\"\\nTuned Test Accuracy: {accuracy_score(y_test, y_pred_tuned):.4f}\")\n",
    "print(f\"Tuned Test AUC-ROC: {roc_auc_score(y_test, y_proba_tuned):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Clustering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering features (engagement-focused)\n",
    "clustering_features = [\n",
    "    'total_clicks', 'avg_daily_clicks', 'active_days', 'unique_resources',\n",
    "    'early_clicks', 'early_active_days', 'early_resources', 'pre_course_clicks',\n",
    "    'avg_score', 'num_assessments_submitted', 'avg_days_early',\n",
    "    'clicks_per_active_day', 'engagement_span'\n",
    "]\n",
    "\n",
    "clustering_features = [f for f in clustering_features if f in df_encoded.columns]\n",
    "print(f\"Clustering features: {len(clustering_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare\n",
    "X_cluster = df_encoded[clustering_features].copy()\n",
    "X_cluster = X_cluster.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "scaler_cluster = StandardScaler()\n",
    "X_cluster_scaled = scaler_cluster.fit_transform(X_cluster)\n",
    "\n",
    "print(f\"Clustering data: {X_cluster_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal k\n",
    "print(\"=\"*60)\n",
    "print(\"FINDING OPTIMAL K\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "K_range = range(2, 11)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "davies_bouldin_scores = []\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_cluster_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_cluster_scaled, kmeans.labels_))\n",
    "    davies_bouldin_scores.append(davies_bouldin_score(X_cluster_scaled, kmeans.labels_))\n",
    "    print(f\"k={k}: Silhouette={silhouette_scores[-1]:.4f}, DB={davies_bouldin_scores[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(K_range, inertias, 'bo-', linewidth=2)\n",
    "axes[0].set_xlabel('k')\n",
    "axes[0].set_ylabel('Inertia')\n",
    "axes[0].set_title('Elbow Method', fontsize=12, fontweight='bold')\n",
    "\n",
    "axes[1].plot(K_range, silhouette_scores, 'go-', linewidth=2)\n",
    "axes[1].set_xlabel('k')\n",
    "axes[1].set_ylabel('Silhouette')\n",
    "axes[1].set_title('Silhouette (higher=better)', fontsize=12, fontweight='bold')\n",
    "\n",
    "axes[2].plot(K_range, davies_bouldin_scores, 'ro-', linewidth=2)\n",
    "axes[2].set_xlabel('k')\n",
    "axes[2].set_ylabel('Davies-Bouldin')\n",
    "axes[2].set_title('Davies-Bouldin (lower=better)', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final K-Means\n",
    "OPTIMAL_K = 4  # Adjust based on above\n",
    "\n",
    "print(f\"\\nFitting K-Means with k={OPTIMAL_K}\")\n",
    "kmeans_final = KMeans(n_clusters=OPTIMAL_K, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans_final.fit_predict(X_cluster_scaled)\n",
    "\n",
    "df_encoded['cluster'] = cluster_labels\n",
    "\n",
    "print(f\"\\nCluster distribution:\")\n",
    "print(df_encoded['cluster'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster profiling\n",
    "print(\"=\"*60)\n",
    "print(\"CLUSTER PROFILES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cluster_profiles = df_encoded.groupby('cluster')[clustering_features].mean()\n",
    "print(cluster_profiles.round(2).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster vs outcome\n",
    "print(\"\\n--- Cluster vs Final Result ---\")\n",
    "cluster_outcome = pd.crosstab(df_encoded['cluster'], df_encoded['final_result'], normalize='index') * 100\n",
    "print(cluster_outcome.round(1))\n",
    "\n",
    "success_rate = df_encoded.groupby('cluster')['target_binary'].mean() * 100\n",
    "print(\"\\nSuccess Rate per Cluster:\")\n",
    "for c, rate in success_rate.items():\n",
    "    print(f\"  Cluster {c}: {rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cluster outcomes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "cluster_outcome[['Pass', 'Distinction', 'Fail', 'Withdrawn']].plot(\n",
    "    kind='bar', stacked=True, ax=axes[0],\n",
    "    color=['#2ecc71', '#3498db', '#e74c3c', '#95a5a6'])\n",
    "axes[0].set_title('Final Result by Cluster', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Cluster')\n",
    "axes[0].set_ylabel('Percentage')\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "success_rate.plot(kind='bar', ax=axes[1], color='steelblue', edgecolor='black')\n",
    "axes[1].set_title('Success Rate by Cluster', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Cluster')\n",
    "axes[1].set_ylabel('Success Rate (%)')\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "axes[1].axhline(y=df_encoded['target_binary'].mean()*100, color='red', linestyle='--', label='Overall')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_cluster_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='viridis', alpha=0.5, s=10)\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "plt.title('Student Clusters (PCA)', fontsize=14, fontweight='bold')\n",
    "\n",
    "centers_pca = pca.transform(kmeans_final.cluster_centers_)\n",
    "plt.scatter(centers_pca[:, 0], centers_pca[:, 1], c='red', marker='X', s=300, edgecolors='black', linewidths=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation (CRISP-DM Phase 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n--- PREDICTIVE MODEL ---\")\n",
    "print(f\"Best: Random Forest (Tuned)\")\n",
    "print(f\"Test AUC-ROC: {roc_auc_score(y_test, y_proba_tuned):.4f}\")\n",
    "print(f\"\\nTop 5 Features:\")\n",
    "for _, row in feature_importance.head(5).iterrows():\n",
    "    print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "print(\"\\n--- CLUSTERING MODEL ---\")\n",
    "print(f\"Clusters: {OPTIMAL_K}\")\n",
    "print(f\"Silhouette: {silhouette_score(X_cluster_scaled, cluster_labels):.4f}\")\n",
    "print(f\"\\nCluster Success Rates:\")\n",
    "for c, rate in success_rate.items():\n",
    "    print(f\"  Cluster {c}: {rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deployment Considerations (CRISP-DM Phase 6)\n",
    "\n",
    "**Recommendations:**\n",
    "1. Deploy Random Forest model for at-risk student identification\n",
    "2. Run predictions weekly during term\n",
    "3. Flag students with P(success) < 0.5 for intervention\n",
    "4. Use cluster assignments for personalised support pathways\n",
    "\n",
    "**Limitations:**\n",
    "- Model trained on historical data\n",
    "- Need validation on new presentations\n",
    "- Consider temporal models for early prediction\n",
    "\n",
    "---\n",
    "\n",
    "**Questions for Amir:**\n",
    "1. Can we use statistical methods (Pearson correlation) beyond module content?\n",
    "2. Does everyone need to present?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
